# FlowUS - Reditor de Laudos de Ultrassonografia
## Com GeraÃ§Ã£o DinÃ¢mica de Laudos via IA Local

AplicaÃ§Ã£o moderna para geraÃ§Ã£o de laudos de ultrassonografia com suporte a **IA local** (Ollama) para gerar textos profissionais e dinÃ¢micos, sem depender de APIs externas.

---

## âœ¨ Features

- âœ… **AutenticaÃ§Ã£o & RBAC**: Sistema completo de login, gestÃ£o de usuÃ¡rios e papÃ©is
- âœ… **IA Local (Ollama)**: Gera laudos dinÃ¢micos localmente, sem internet
- âœ… **MÃºltiplos Tipos de Exame**: Abdominal, Tireoide, PÃ©lvico, Mamas, etc.
- âœ… **Achados CustomizÃ¡veis**: SeleÃ§Ã£o de findings com tamanho, lateralidade e quantidade
- âœ… **EdiÃ§Ã£o de Laudos**: Revise e ajuste laudos gerados pela IA
- âœ… **GeraÃ§Ã£o PDF Profissional**: PDFs com header, footer, assinatura digital
- âœ… **Meus Laudos**: HistÃ³ rico e gerenciamento de laudos salvos
- âœ… **ConfiguraÃ§Ãµes da Unidade**: Dados da clÃ­nica, cores, logo
- âœ… **Tema Claro/Escuro**: Interface responsiva e moderna
- âœ… **Offline First**: Tudo funciona localmente com localStorage

---

## ğŸš€ Quick Start

### PrÃ©-requisitos
- Node.js 18+
- **Ollama** (para usar IA local) - [Instalar](https://ollama.ai)

### 1. InstalaÃ§Ã£o

```bash
git clone https://github.com/bjmvictor/RLU-Reditor-de-laudos-US.git
cd "FlowUS - Reditor de Laudos"
npm install
```

### 2. Setup de IA Local (Ollama)

```bash
# 1. Instale Ollama em https://ollama.ai
# 2. Abra PowerShell/Terminal e execute:
ollama pull mistral

# 3. Inicie o servidor Ollama (deixe rodando):
ollama serve

# 4. Em outra janela, inicie o dev server:
npm run dev
```

**Nota**: O servidor Ollama precisa estar rodando (`ollama serve`) enquanto vocÃª usa o FlowUS!

### 3. Desenvolvimento

```bash
npm run dev          # Inicia em http://localhost:5173
npm run build        # Build para produÃ§Ã£o
npm run preview      # Preview do build
```

---

## ğŸ” Login PadrÃ£o

**UsuÃ¡rio**: `admin`  
**Senha**: `admin`

Use este acesso para gerenciar usuÃ¡rios, papÃ©is e configuraÃ§Ãµes.

---

## ğŸ“– Como Usar a IA Local

### Gerar Laudo com IA

1. **Selecione o tipo de exame** (Abdominal Total, Tireoide, etc.)
2. **Insira dados do paciente** (nome, idade, gÃªnero, indicaÃ§Ã£o clÃ­nica)
3. **Selecione os achados** (Normal, nÃ³dulos, cÃ¡lculos, etc.)
4. **Clique "Gerar Laudo com IA Local"**
   - A IA local processarÃ¡ e gerarÃ¡ um laudo profissional
   - Sem dependÃªncia de internet
   - Sem envio de dados para servidor externo

### Editar Laudo

O laudo gerado pode ser editado no campo de texto antes de salvar ou exportar PDF.

### Salvar e Exportar

- **Salvar**: Armazena na seÃ§Ã£o "Meus Laudos" (localStorage)
- **Baixar PDF**: Gera PDF profissional com cabeÃ§alho da unidade, assinatura digital e pÃ¡gina

---

## ğŸ¤– ConfiguraÃ§Ã£o da IA Local

### Modelos DisponÃ­veis

VocÃª pode trocar de modelo alterando `VITE_OLLAMA_MODEL` no `.env.local`:

| Modelo | Tamanho | Velocidade | Qualidade | PortuguÃªs |
|--------|---------|-----------|-----------|-----------|
| **mistral** (padrÃ£o) | 4.1GB | âš¡âš¡ | â­â­â­â­ | â­â­â­â­ |
| neural-chat | 4.7GB | âš¡âš¡âš¡ | â­â­â­ | â­â­â­â­ |
| openchat | 3.8GB | âš¡âš¡âš¡ | â­â­â­ | â­â­â­ |
| zephyr | 4.2GB | âš¡âš¡ | â­â­â­â­ | â­â­â­â­ |

### Instalar Novo Modelo

```bash
ollama pull <nome-do-modelo>
ollama list  # Ver modelos instalados
```

### Customizar Comportamento

Edite `src/utils/aiReportGenerator.ts` para ajustar:
- `temperature`: Criatividade (0-1, padrÃ£o 0.6)
- `num_predict`: Tamanho mÃ¡ximo do laudo (padrÃ£o 1000)
- `top_k`, `top_p`: Variedade e coerÃªncia
- `repeat_penalty`: Reduzir repetiÃ§Ã£o

---

## ğŸ“ Estrutura do Projeto

```
src/
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ Login.tsx                    # PÃ¡gina de autenticaÃ§Ã£o
â”‚   â”œâ”€â”€ Index.tsx                    # Home
â”‚   â”œâ”€â”€ UltrasoundReportGenerator.tsx # Editor de laudos (Principal)
â”‚   â”œâ”€â”€ UsersManagement.tsx          # Gerenciamento de usuÃ¡rios
â”‚   â”œâ”€â”€ RolesManagement.tsx          # GestÃ£o de papÃ©is e permissÃµes
â”‚   â””â”€â”€ Settings.tsx                 # ConfiguraÃ§Ãµes da unidade
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ Navbar.tsx                   # NavegaÃ§Ã£o
â”‚   â”œâ”€â”€ ProtectedRoute.tsx           # Rotas protegidas por permissÃ£o
â”‚   â””â”€â”€ AIReportGenerator.tsx        # Componente de IA (NOVO!)
â”œâ”€â”€ contexts/
â”‚   â””â”€â”€ AuthContext.tsx              # Context de autenticaÃ§Ã£o
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ auth.ts                      # LÃ³gica de autenticaÃ§Ã£o
â”‚   â”œâ”€â”€ storage.ts                   # PersistÃªncia de laudos
â”‚   â””â”€â”€ aiReportGenerator.ts         # IA local com Ollama (NOVO!)
â””â”€â”€ types/
    â””â”€â”€ index.ts                     # Types TypeScript
```

---

## ğŸ”§ VariÃ¡veis de Ambiente

Crie um arquivo `.env.local` na raiz do projeto:

```env
# Ollama
VITE_OLLAMA_MODEL=mistral
```

Se deixar vazio ou nÃ£o usar Ollama, o app utilizarÃ¡ templates automÃ¡ticos.

---

## ğŸ”’ SeguranÃ§a & Privacidade

âœ… **Dados Locais**: Tudo Ã© armazenado no localStorage do navegador  
âœ… **Sem Upload**: Nenhuma informaÃ§Ã£o de paciente Ã© enviada para a internet  
âœ… **IA Offline**: Ollama roda localmente, sem dependÃªncia de APIs  
âœ… **LGPD Compliant**: Perfeito para dados sensÃ­veis de saÃºde  
âœ… **Sem Rastreamento**: Sem analytics ou coleta de dados

---

## ğŸ†˜ SoluÃ§Ã£o de Problemas

### "Ollama nÃ£o estÃ¡ disponÃ­vel"

```bash
# Verifique se Ollama estÃ¡ rodando
curl http://localhost:11434/api/tags

# Se nÃ£o responder, inicie:
ollama serve
```

### GeraÃ§Ã£o de IA muito lenta

- Use um modelo menor: `ollama pull neural-chat`
- Aumente a RAM alocada (Ollama usa atÃ© a RAM disponÃ­vel)
- Reduza `num_predict` em `aiReportGenerator.ts`

### Modelo nÃ£o encontrado

```bash
ollama list              # Ver modelos instalados
ollama pull mistral      # Baixar modelo
```

---

## ğŸ“š DocumentaÃ§Ã£o Adicional

- **[OLLAMA_SETUP.md](./OLLAMA_SETUP.md)** - Guia completo de configuraÃ§Ã£o da IA local
- **Modelos de Laudos**: Baseados em [CompÃªndio da Radiologia](https://sites.google.com/site/compendiodaradiologia/)

---

## ğŸ“¦ Tech Stack

- **React 18.3** + TypeScript
- **Vite 6.4** - Build tool
- **Tailwind CSS** - Styling
- **shadcn/ui** - Components
- **jsPDF** - PDF generation
- **Ollama** - Local AI (opcional)
- **React Router** - Navigation
- **Sonner** - Toast notifications

---

## ğŸš¢ Deploy

### Build para ProduÃ§Ã£o

```bash
npm run build      # Cria pasta dist/
npm run preview    # Testa o build localmente
```

FaÃ§a upload da pasta `dist/` para seu servidor.

### ObservaÃ§Ã£o Importante

Se usar em produÃ§Ã£o, vocÃª precisarÃ¡ de um servidor Ollama rodando separadamente. Considere:

1. **Servidor Ollama em mÃ¡quina servidor**: Acesse via IP interno
2. **Ou**: Use API de terceiros (OpenAI, Hugging Face) como fallback
3. **Ou**: Apenas templates automÃ¡ticos (sem IA)

---

## ğŸ‘¥ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Abra uma issue ou PR com suas ideias.

---

## ğŸ“„ LicenÃ§a

MIT

---

## ğŸ“ Suporte

Para problemas ou dÃºvidas:

1. Verifique o arquivo [OLLAMA_SETUP.md](./OLLAMA_SETUP.md)
2. Abra uma issue no GitHub
3. Consulte a documentaÃ§Ã£o do [Ollama](https://ollama.ai)

---

**Desenvolvido com â¤ï¸ para radiologistas que querem eficiÃªncia, privacidade e IA local.**
